package services

import (
	"sync"
	"time"
)

// MemoryServiceCache implements an in-memory service discovery cache
type MemoryServiceCache struct {
	items   map[string]*cacheEntry
	lruList *cacheList
	maxSize int
	stats   *serviceCacheStats
	mu      sync.RWMutex
}

// cacheEntry represents a cached service discovery result
type cacheEntry struct {
	key       string
	instances []*ServiceInstance
	expiry    time.Time
	accessed  time.Time

	// LRU list pointers
	prev *cacheEntry
	next *cacheEntry
}

// cacheList manages the LRU ordering
type cacheList struct {
	head *cacheEntry
	tail *cacheEntry
	size int
}

// serviceCacheStats tracks cache performance
type serviceCacheStats struct {
	hits      int64
	misses    int64
	sets      int64
	deletes   int64
	evictions int64
	size      int64
	mu        sync.RWMutex
}

// NewMemoryServiceCache creates a new in-memory service cache
func NewMemoryServiceCache() ServiceCache {
	return &MemoryServiceCache{
		items:   make(map[string]*cacheEntry),
		lruList: &cacheList{},
		maxSize: 1000, // Default max entries
		stats:   &serviceCacheStats{},
	}
}

// NewMemoryServiceCacheWithSize creates a cache with specified max size
func NewMemoryServiceCacheWithSize(maxSize int) ServiceCache {
	return &MemoryServiceCache{
		items:   make(map[string]*cacheEntry),
		lruList: &cacheList{},
		maxSize: maxSize,
		stats:   &serviceCacheStats{},
	}
}

// Get retrieves service instances from cache
func (c *MemoryServiceCache) Get(key string) ([]*ServiceInstance, bool) {
	c.mu.RLock()
	entry, exists := c.items[key]
	c.mu.RUnlock()

	if !exists {
		c.recordMiss()
		return nil, false
	}

	// Check if expired
	if time.Now().After(entry.expiry) {
		c.mu.Lock()
		c.removeEntry(key)
		c.mu.Unlock()
		c.recordMiss()
		return nil, false
	}

	// Update access time and move to front
	c.mu.Lock()
	entry.accessed = time.Now()
	c.lruList.moveToFront(entry)
	c.mu.Unlock()

	c.recordHit()

	// Return a copy to prevent external modification
	instances := make([]*ServiceInstance, len(entry.instances))
	copy(instances, entry.instances)

	return instances, true
}

// Set stores service instances in cache
func (c *MemoryServiceCache) Set(key string, instances []*ServiceInstance, ttl time.Duration) {
	if ttl <= 0 {
		ttl = 5 * time.Minute // Default TTL
	}

	expiry := time.Now().Add(ttl)

	c.mu.Lock()
	defer c.mu.Unlock()

	// Check if entry already exists
	if existing, exists := c.items[key]; exists {
		// Update existing entry
		existing.instances = c.copyInstances(instances)
		existing.expiry = expiry
		existing.accessed = time.Now()
		c.lruList.moveToFront(existing)
		c.recordSet()
		return
	}

	// Create new entry
	entry := &cacheEntry{
		key:       key,
		instances: c.copyInstances(instances),
		expiry:    expiry,
		accessed:  time.Now(),
	}

	// Check if we need to evict entries
	for c.lruList.size >= c.maxSize {
		c.evictLRU()
	}

	// Add to cache
	c.items[key] = entry
	c.lruList.addToFront(entry)
	c.recordSet()
}

// Delete removes an entry from cache
func (c *MemoryServiceCache) Delete(key string) {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.removeEntry(key)
	c.recordDelete()
}

// Clear removes all entries from cache
func (c *MemoryServiceCache) Clear() {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.items = make(map[string]*cacheEntry)
	c.lruList = &cacheList{}

	c.stats.mu.Lock()
	c.stats.size = 0
	c.stats.mu.Unlock()
}

// Stats returns cache statistics
func (c *MemoryServiceCache) Stats() CacheStats {
	c.stats.mu.RLock()
	defer c.stats.mu.RUnlock()

	total := c.stats.hits + c.stats.misses
	hitRate := 0.0
	if total > 0 {
		hitRate = float64(c.stats.hits) / float64(total)
	}

	return CacheStats{
		Hits:    c.stats.hits,
		Misses:  c.stats.misses,
		Sets:    c.stats.sets,
		Deletes: c.stats.deletes,
		HitRate: hitRate,
		Size:    c.stats.size,
		Memory:  c.estimateMemoryUsage(),
	}
}

// removeEntry removes an entry from cache (must be called with lock held)
func (c *MemoryServiceCache) removeEntry(key string) {
	if entry, exists := c.items[key]; exists {
		delete(c.items, key)
		c.lruList.remove(entry)

		c.stats.mu.Lock()
		c.stats.size--
		c.stats.mu.Unlock()
	}
}

// evictLRU evicts the least recently used entry
func (c *MemoryServiceCache) evictLRU() {
	if c.lruList.tail != nil {
		c.removeEntry(c.lruList.tail.key)

		c.stats.mu.Lock()
		c.stats.evictions++
		c.stats.mu.Unlock()
	}
}

// copyInstances creates a deep copy of service instances
func (c *MemoryServiceCache) copyInstances(instances []*ServiceInstance) []*ServiceInstance {
	copied := make([]*ServiceInstance, len(instances))
	for i, instance := range instances {
		// Create a copy of the instance
		copied[i] = &ServiceInstance{
			ID:          instance.ID,
			ServiceName: instance.ServiceName,
			Version:     instance.Version,
			Endpoint:    instance.Endpoint,
			Health:      instance.Health,
			Metadata:    make(map[string]string),
			TenantID:    instance.TenantID,
			Weight:      instance.Weight,
			LastSeen:    instance.LastSeen,
		}

		// Copy metadata
		for k, v := range instance.Metadata {
			copied[i].Metadata[k] = v
		}
	}
	return copied
}

// estimateMemoryUsage estimates the memory usage of the cache
func (c *MemoryServiceCache) estimateMemoryUsage() int64 {
	c.mu.RLock()
	defer c.mu.RUnlock()

	var totalSize int64

	for _, entry := range c.items {
		// Estimate size of key
		totalSize += int64(len(entry.key))

		// Estimate size of instances
		for _, instance := range entry.instances {
			totalSize += int64(len(instance.ID))
			totalSize += int64(len(instance.ServiceName))
			totalSize += int64(len(instance.Version))
			totalSize += int64(len(instance.Endpoint.Host))
			totalSize += int64(len(instance.TenantID))

			// Estimate metadata size
			for k, v := range instance.Metadata {
				totalSize += int64(len(k) + len(v))
			}

			// Add fixed overhead per instance
			totalSize += 200 // Approximate overhead
		}

		// Add fixed overhead per entry
		totalSize += 100
	}

	return totalSize
}

// Metrics recording methods
func (c *MemoryServiceCache) recordHit() {
	c.stats.mu.Lock()
	c.stats.hits++
	c.stats.mu.Unlock()
}

func (c *MemoryServiceCache) recordMiss() {
	c.stats.mu.Lock()
	c.stats.misses++
	c.stats.mu.Unlock()
}

func (c *MemoryServiceCache) recordSet() {
	c.stats.mu.Lock()
	c.stats.sets++
	c.stats.size++
	c.stats.mu.Unlock()
}

func (c *MemoryServiceCache) recordDelete() {
	c.stats.mu.Lock()
	c.stats.deletes++
	c.stats.mu.Unlock()
}

// LRU list methods

// addToFront adds an entry to the front of the LRU list
func (l *cacheList) addToFront(entry *cacheEntry) {
	if l.head == nil {
		l.head = entry
		l.tail = entry
	} else {
		entry.next = l.head
		l.head.prev = entry
		l.head = entry
	}
	l.size++
}

// remove removes an entry from the LRU list
func (l *cacheList) remove(entry *cacheEntry) {
	if entry.prev != nil {
		entry.prev.next = entry.next
	} else {
		l.head = entry.next
	}

	if entry.next != nil {
		entry.next.prev = entry.prev
	} else {
		l.tail = entry.prev
	}

	entry.prev = nil
	entry.next = nil
	l.size--
}

// moveToFront moves an entry to the front of the LRU list
func (l *cacheList) moveToFront(entry *cacheEntry) {
	if l.head == entry {
		return // Already at front
	}

	l.remove(entry)
	l.addToFront(entry)
}

// TTLServiceCache wraps a cache with automatic TTL cleanup
type TTLServiceCache struct {
	delegate        ServiceCache
	cleanupInterval time.Duration
	stopCh          chan struct{}
}

// NewTTLServiceCache creates a cache with automatic TTL cleanup
func NewTTLServiceCache(delegate ServiceCache, cleanupInterval time.Duration) *TTLServiceCache {
	cache := &TTLServiceCache{
		delegate:        delegate,
		cleanupInterval: cleanupInterval,
		stopCh:          make(chan struct{}),
	}

	// Start cleanup goroutine
	go cache.cleanupLoop()

	return cache
}

// Get delegates to the underlying cache
func (t *TTLServiceCache) Get(key string) ([]*ServiceInstance, bool) {
	return t.delegate.Get(key)
}

// Set delegates to the underlying cache
func (t *TTLServiceCache) Set(key string, instances []*ServiceInstance, ttl time.Duration) {
	t.delegate.Set(key, instances, ttl)
}

// Delete delegates to the underlying cache
func (t *TTLServiceCache) Delete(key string) {
	t.delegate.Delete(key)
}

// Clear delegates to the underlying cache
func (t *TTLServiceCache) Clear() {
	t.delegate.Clear()
}

// Stats delegates to the underlying cache
func (t *TTLServiceCache) Stats() CacheStats {
	return t.delegate.Stats()
}

// Close stops the cleanup goroutine
func (t *TTLServiceCache) Close() {
	close(t.stopCh)
}

// cleanupLoop runs periodic cleanup of expired entries
func (t *TTLServiceCache) cleanupLoop() {
	ticker := time.NewTicker(t.cleanupInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// For now, this is a no-op since the underlying cache
			// handles expiry on access. In a more sophisticated
			// implementation, we could track expiry times and
			// proactively clean up expired entries.
		case <-t.stopCh:
			return
		}
	}
}

// MultiTierServiceCache implements a multi-tier caching strategy
type MultiTierServiceCache struct {
	l1Cache ServiceCache // Fast, small cache (e.g., in-memory)
	l2Cache ServiceCache // Slower, larger cache (e.g., Redis)
	stats   *multiTierStats
}

// multiTierStats tracks multi-tier cache performance
type multiTierStats struct {
	l1Hits   int64
	l1Misses int64
	l2Hits   int64
	l2Misses int64
	mu       sync.RWMutex
}

// NewMultiTierServiceCache creates a multi-tier cache
func NewMultiTierServiceCache(l1Cache, l2Cache ServiceCache) *MultiTierServiceCache {
	return &MultiTierServiceCache{
		l1Cache: l1Cache,
		l2Cache: l2Cache,
		stats:   &multiTierStats{},
	}
}

// Get retrieves from L1 cache first, then L2 cache
func (m *MultiTierServiceCache) Get(key string) ([]*ServiceInstance, bool) {
	// Try L1 cache first
	if instances, found := m.l1Cache.Get(key); found {
		m.stats.mu.Lock()
		m.stats.l1Hits++
		m.stats.mu.Unlock()
		return instances, true
	}

	m.stats.mu.Lock()
	m.stats.l1Misses++
	m.stats.mu.Unlock()

	// Try L2 cache
	if instances, found := m.l2Cache.Get(key); found {
		// Promote to L1 cache
		m.l1Cache.Set(key, instances, 1*time.Minute) // Shorter TTL for L1

		m.stats.mu.Lock()
		m.stats.l2Hits++
		m.stats.mu.Unlock()
		return instances, true
	}

	m.stats.mu.Lock()
	m.stats.l2Misses++
	m.stats.mu.Unlock()

	return nil, false
}

// Set stores in both L1 and L2 caches
func (m *MultiTierServiceCache) Set(key string, instances []*ServiceInstance, ttl time.Duration) {
	// Store in L1 with shorter TTL
	l1TTL := ttl
	if l1TTL > 5*time.Minute {
		l1TTL = 5 * time.Minute
	}
	m.l1Cache.Set(key, instances, l1TTL)

	// Store in L2 with full TTL
	m.l2Cache.Set(key, instances, ttl)
}

// Delete removes from both caches
func (m *MultiTierServiceCache) Delete(key string) {
	m.l1Cache.Delete(key)
	m.l2Cache.Delete(key)
}

// Clear clears both caches
func (m *MultiTierServiceCache) Clear() {
	m.l1Cache.Clear()
	m.l2Cache.Clear()
}

// Stats returns combined statistics
func (m *MultiTierServiceCache) Stats() CacheStats {
	l1Stats := m.l1Cache.Stats()
	l2Stats := m.l2Cache.Stats()

	m.stats.mu.RLock()
	defer m.stats.mu.RUnlock()

	totalHits := m.stats.l1Hits + m.stats.l2Hits
	totalMisses := m.stats.l1Misses + m.stats.l2Misses
	total := totalHits + totalMisses

	hitRate := 0.0
	if total > 0 {
		hitRate = float64(totalHits) / float64(total)
	}

	return CacheStats{
		Hits:    totalHits,
		Misses:  totalMisses,
		Sets:    l1Stats.Sets + l2Stats.Sets,
		Deletes: l1Stats.Deletes + l2Stats.Deletes,
		HitRate: hitRate,
		Size:    l1Stats.Size + l2Stats.Size,
		Memory:  l1Stats.Memory + l2Stats.Memory,
	}
}
